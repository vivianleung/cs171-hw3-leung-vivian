<h1>Design Studio 2</h1>
<h2>Part 1: Analysis</h2>

1. What trends do you see in the data?
	
	Initially, population estimate values remain relatively stable at low numbers, increasing only a little over 2-fold over 1700 years. Between 1700 and 1950, values increase exponentially, with more than a 4-fold change over the 250-year time period. From 1950 to 2050, population increases relatively linearly (about a 3.6-fold change over 100 years) with rate deceleration over the final 30 years. 

2. Analyze how big the differences between various estimates are. Do you see a trend, i.e., do the differences become smaller or larger over time? Think about these differences relative to the estimates at the respective time points and in absolute terms. When are the uncertainties the largest in absolute, when in relative terms?

	Absolute differences in estimates are relatively moderate. Between 1900 and 2000, the differences are relatively low. However, after 2000, the differences are relatively large, likely due to the larger values. However, looking at them in proportion to the value's magnitude, differences are larger the earlier the estimates are, except for the 20th century, which remain relatively minimal. Over time, estimates tend to converge, meaning percent differences of each estimate source generally decrease over time. Uncertainties are thus absolutely largest in more recent times, and relatively largest in earlier times.

4. Do you think you can faithfully represent the uncertainty and the data in the same plot? Why, or why not? 

	Not really, as we need to control for the differences in magnitude over time (ie. by normalizing), while also representing the change in magnitude over time. We can plot the percent change of population for each estimate (relative to the previous year) could be plotted over time, and this would show trends in changes in magnitude of the population as well as the uncertainty of the data (according to the visual divergence of estimates), but we would also then lose the raw value of the data.	 

5. What effect do you think will the linear interpolation have on the uncertainty? 
	Statistical uncertainty, or the standard deviation, will likely decrease with linear interpolation as we are increasing the number of data points. (This is assuming that the estimates we are interpolating are relatively close together, i.e. no outliers).

6. Is linear interpolation a suitable method for this data?
	Perhaps for the earlier portion of the data (pre-1700), but because we see exponential growth in later times, linear interpolation would not be as accurate as, for example, interpolating on an exponential scale. However, if we want to represent the "uncertainty" (i.e. percent difference from consensus), then interpolation is not a suitable method to apply to the data. In this case, interpolation would skew our uncertainties, and misleadingly represent the real data (what the sources are actually estimating).
	

<h2>Part 2: Sketching</h2>

![Alt text](/Problem2/differenceGraph.png "From HW3 Problem 2 sketch:
")

<h2>Part 3: Reflection</h2>


